import os
import json
import sqlite3


# File containing JSON paths
JSON_PATHS_FILE = "json_paths.txt"

# Database name
DATABASE_NAME = "maniskill.db"


# Function to load JSON metadata
def load_metadata(file_path):
    with open(file_path, "r") as f:
        return json.load(f)


def extract_task_info(metadata):
    """
    Extracts task-level metadata (env_id, max_episode_steps, env_kwargs).
    Sample for format:
    "env_info": {
        "env_id": "AnymalC-Reach-v1",
        "env_kwargs": {
            "num_envs": 1024,
            "reconfiguration_freq": 1,
            "human_render_camera_configs": {
                "shader_pack": "default"
        },
            "obs_mode": "state",
            "render_mode": "rgb_array",
            "sim_backend": "physx_cuda",
            "control_mode": "pd_joint_delta_pos"
        },
        "max_episode_steps": 200
    }
    """

    env_info = metadata.get("env_info", {})
    env_id = env_info.get("env_id", "UnknownTask")
    max_episode_steps = env_info.get("max_episode_steps", None)
    env_kwargs = json.dumps(env_info.get("env_kwargs", {}))

    return {"env_id": env_id, "max_episode_steps": max_episode_steps, "env_kwargs": env_kwargs}

def extract_source_info(metadata):
    """
    Extracts source type and description.
    Sample for format:
    "source_type": "rl",
    "source_desc": "Demonstrations generated by rolling out a PPO dense reward trained policy"
    """
    env_id = metadata.get("env_info", {}).get("env_id", "UnknownTask")
    source_type = metadata.get("source_type", "Unknown")
    source_desc = metadata.get("source_desc", "No description")

    return {"env_id": env_id, "source_type": source_type, "source_desc": source_desc}

def extract_episodes(metadata, env_id):
    """
    Extracts episodes and formats them for insertion.
    Sample for format:
    "episodes": [
    {
      "episode_id": 0,
      "episode_seed": 40194941,
      "control_mode": "pd_joint_delta_pos",
      "elapsed_steps": 200,
      "reset_kwargs": {},
      "success": true,
      "fail": false
    }
    ]
    """
    episodes = metadata.get("episodes", [])
    parsed_episodes = []

    for episode in episodes:
        parsed_episodes.append({
            "episode_id": episode.get("episode_id"),
            "env_id": env_id,
            "episode_seed": episode.get("episode_seed"),
            "reset_kwargs": json.dumps(episode.get("reset_kwargs", {})),  # Convert dict to json string
            "control_mode": episode.get("control_mode", ""),
            "elapsed_steps": episode.get("elapsed_steps", 0),
            "success": int(episode.get("success", False)),
            "fail": int(episode.get("fail", False))
        })

    return parsed_episodes


def create_database():
    """
    Creates database tables if they don't exist.
    """
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()

    # Create tables
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS task_info (
            env_id TEXT PRIMARY KEY,
            max_episode_steps INTEGER,
            env_kwargs TEXT
        )
    """)

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS source_info (
            env_id TEXT PRIMARY KEY,
            source_type TEXT,
            source_desc TEXT
        )
    """)

    conn.commit()
    conn.close()
    print("Database created.")


def save_to_database(task_info, episodes, source_info):
    """
    Save metadata to the database tables.
    """
    conn = sqlite3.connect("maniskill.db")
    cursor = conn.cursor()

    env_id = task_info["env_id"]

    # Store task info
    cursor.execute("""
        INSERT OR REPLACE INTO task_info (env_id, max_episode_steps, env_kwargs) 
        VALUES (?, ?, ?)
    """, (env_id, task_info["max_episode_steps"], task_info["env_kwargs"]))

    # Store source info
    cursor.execute("""
        INSERT OR REPLACE INTO source_info (env_id, source_type, source_desc) 
        VALUES (?, ?, ?)
    """, (env_id, source_info["source_type"], source_info["source_desc"]))

    # Change env_id name so it can be saved into the SQLite table
    safe_env_id = env_id.replace("-", "_")

    # Create new table for each env_id
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS episodes_{safe_env_id} (
            episode_id INTEGER PRIMARY KEY,
            episode_seed INTEGER,
            reset_kwargs TEXT,
            control_mode TEXT,
            elapsed_steps INTEGER,
            success INTEGER,
            fail INTEGER
        )
    """)

    # Insert episodes
    for episode in episodes:
        reset_kwargs = episode.get("reset_kwargs", {})  # Default to empty dict if missing
        control_mode = episode.get("control_mode", "")

        # Convert reset_kwargs and control_mode to JSON strings if needed
        reset_kwargs_str = json.dumps(reset_kwargs) if isinstance(reset_kwargs, dict) else str(reset_kwargs)
        control_mode_str = json.dumps(control_mode) if isinstance(control_mode, dict) else str(control_mode)

        cursor.execute(f"""
            INSERT OR REPLACE INTO episodes_{safe_env_id} (
                episode_id, episode_seed, reset_kwargs, control_mode, elapsed_steps, success, fail
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            episode["episode_id"], episode["episode_seed"], reset_kwargs_str,
            control_mode_str, episode["elapsed_steps"], episode["success"], episode["fail"]
        ))

    conn.commit()
    conn.close()
    print(f"Saved {len(episodes)} episodes for env_id: {env_id}")


def process_json_paths():
    """ 
    Reads json paths from json_paths.txt and processes each file. 
    """
    with open("json_paths.txt", "r") as f:
        json_files = [line.strip() for line in f.readlines()]

    for file_path in json_files:
        print(f"\nProcessing JSON File: {file_path} ...")

        if os.path.exists(file_path) and file_path.endswith(".json"):
            with open(file_path, "r") as json_file:
                metadata = json.load(json_file)

            # Extract dat
            task_info = extract_task_info(metadata)
            source_info = extract_source_info(metadata)
            episodes = extract_episodes(metadata, task_info["env_id"])

            # Save to DB
            save_to_database(task_info, episodes, source_info)

# Run the script
if __name__ == "__main__":
    create_database()
    process_json_paths()
    print("All metadata files processed successfully!")